%
\documentclass[runningheads]{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
%\frontmatter          % for the preliminaries
%
%\pagestyle{headings}  % switches on printing of running heads
%
%
%\mainmatter              % start of the contributions
%
\title{Online Outlier Detection over Large Datasets}
\titlerunning{Online Outlier Detection}  % abbreviated title (for running head)
\subtitle{Seminar - Multimedia Retrieval and Data Mining}
%
\author{Matthias Hansen}
\authorrunning{Hansen} % abbreviated author list (for running head)
%
%
\institute{Data Management and Data Exploration Group\\
RWTH Aachen University\\
Germany\\
\email{matthias.hansen@rwth-aachen.de}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Performing Distance-Based Outlier Analysis on Large Datasets poses a challenge with respect to computation time. Due to its $n\log(n)$ complexity, the kd-tree based algorithm for $k$-nearest neighbor search is not applicable for interactive outlier exploration over data sets whose sizes ranges in the millions. In this paper, we present a system that preprocesses the results of an initial application of the standard algorithm and stores them in memory to efficiently support any following outlier detection queries and also two novel outlier analysis operations that allow data analysts to gain insight into the outlier status of relevant parts of the data set much more quickly than the traditional approach of repeatedly finding $k$-nearest neighbors would.
\end{abstract}

\section{Motivation}
Finding outliers in data is a problem frequently occured in data analysis. For example credit card providers use outlier detection to find transactions that could be fraudulent. Similarly, in a network security context, outlier detection can be used to identify intruders. It has also been used for goals as diverse as data cleaning, financial planning and severe weather prediction.
%TODO: expand this paragraph

\subsection{Outlier Definitions}

There is no universally accepted precise definition of what constitutes an outlier in a data set. Instead, several different definitions are used as appropriate for the task at hand.

Traditionally, outliers have been defined statistically, i.e. with respect to a statistical distribution. In this case, every data point to which the distribution assigns a probability below some threshold is classified as an outlier. However, this is only applicable if those points that are not outliers are known to follow a specific distribution. This is not always the case and choosing a statistical distribution so that it reflects the data set is challenging and may require domain knowledge and--especially for multivariate data knowledge about statistics.

A competing outlier definition is the distance-based outlier. A point is a distance-based outlier if its $\varepsilon$-neighborhood with respect to a distance measure contains less than $k$ points, where $\varepsilon$, $k$ and the distance measure can be chosen as necessary for the task at hand.

\begin{definition}
    Let DB be a set of tuples. A data point $p\in DB$ is an outlier iff 
    $|\{q \in DB | d(p,q) < \varepsilon\}| < k$.
\end{definition}

This definition has is impartial to the distribution of the data and it is especially useful for geospatial data, where the Euclidean distance has additional meaning apart from indicating (dis-)similarity. Drawbacks of this definition include inability to cope with arbitrarily high-dimensional data, since the distances of the most-distant and the least-distant points approach each other as the dimensionality of data set grows--a phenomenon referred to as the curse of dimensionality. %TODO cite that one paper about curse of dimensionality.
%TODO Improve this paragraph?

\subsection{State of the Art}

The naive way to detect distance-based outliers is to simply compute the distances between each pair of points. From the resulting distance matrix, the k-th closest other point for every point $p_i$ can be found by scanning $p_i$'s row in the distance matrix. Then one has to check whether the distance to this k-th closest point is bigger than $\varepsilon$. If this is the case, then $p_i$ is an outlier.

The time and space complexities of this algorithm are both in $\mathcal{O}((n\cdot d)^2)$ ($n$ being the number of rows and $d$ the number of columns in the DB), so the algorithm does not scale well for large amounts of data.

Faster algorithms to solve this problem are usually based on spatial index structures. For example, a kd-tree based algorithm exists that essentially solves this problem in $\mathcal{O}(n\cdot d \cdot\log(n\cdot d))$, by first finding the $k$-nearest neighbors of each $p_i$ and then comparing the distance to the $k$-th nearest neighbor to $\varepsilon$ to find out whether $p_i$ is an outlier. 

This algorithm clearly performs better for large datasets. However in our experiments we found that for e.g. a two-dimensional dataset with . %TODO insert values here

This is not a big problem if only one isolated outlier detection query has to be performed. However, a typical use case for outlier detection is exploratory data analysis, where it is not clear ahead of time whether one set of parameters ($k$,$\varepsilon$) produces the desired result. An analyst might, for example, run an outlier detection query with a value of $k$ that is very high, wait a long time for the result, and then find out that the outlier detection algorithm has classified every point as an outlier. They would then have to re-run the outlier detection algorithm with a different set of parameters, completely recomputing the $k$-nearest neighbors in the process.
%TODO Add experimental results here
%TODO: Also discuss the DOLPHIN System.
\subsection{Considerations for a New Outlier Detection System}


Example citation \cite{DBLP:conf/civr/BeecksUS10}

%
% ---- Bibliography ----
%

\bibliographystyle{abbrv}
\bibliography{references}  

\end{document}
